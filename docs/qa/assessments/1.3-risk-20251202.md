# **Enhanced** Risk Profile: Story 1.3: PostgreSQL Database Schema & Migrations

**Date**: 2025-12-02  
**Reviewer**: Quinn (Senior Test Architect)  
**Status**: Updated & Finalized for Architect Review

## 1. Executive Summary

This enhanced risk profile for Story 1.3 addresses the database schema, which is the backbone of the entire application. The initial assessment correctly identified key risk areas but failed to provide the necessary implementation-level detail. This document rectifies that by providing **prescriptive, copy-paste-ready mitigations** for the Architect Agent.

I have identified **2 CRITICAL** and **4 HIGH** priority risks. The most severe are the risk of **leaking database credentials** and the potential for **catastrophic migration failures**. This assessment also introduces a new high-priority risk concerning long-term schema maintainability ("ENUM Hell"), which, if unaddressed, will lead to significant development friction in the future.

- **Total Risks Identified**: 7
- **Critical Risks (Score 9)**: 2
- **High Risks (Score 6-8)**: 4
- **Medium Risks (Score 4-5)**: 1
- **Overall Assessment**: **NO-GO**. The story is fundamentally unsafe to implement until the Architect Agent integrates these mitigations.

## 2. Detailed Risk Register

| Risk ID | Description | Category | Probability (1-3) | Impact (1-3) | Score (P√óI) | Priority |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **SEC-001** | **Leaked `DATABASE_URL` via Git History or Logs** | Security | 3 (High) | 3 (High) | **9** | **CRITICAL** |
| **OPS-001** | **Catastrophic Migration Failure in Production** | Operational | 3 (High) | 3 (High) | **9** | **CRITICAL** |
| **DATA-001**| **Data Integrity Violation due to Constraint Flaws** | Data | 2 (Medium) | 3 (High) | **6** | **HIGH** |
| **TECH-001**| **Schema Drift Between `schema.prisma` and Database**| Technical | 3 (High) | 2 (Medium) | **6** | **HIGH** |
| **TECH-002**| **"ENUM Hell" Leading to Poor Maintainability** | Technical | 3 (High) | 2 (Medium) | **6** | **HIGH** |
| **PERF-001**| **Inefficient Queries due to Missing/Wrong Indexes** | Performance | 2 (Medium) | 3 (High) | **6** | **HIGH** |
| **OPS-002** | **Accidental Production Seeding Corrupting Data** | Operational | 2 (Medium) | 2 (Medium) | **4** | **MEDIUM** |

---

## 3. Actionable Mitigation & Implementation Plan

**This section is for the Architect Agent.** Copy the following subsections directly into the `Dev Notes` of `1.3.story.md`, replacing the existing `Risk Mitigation Strategies` section.

---

### **CRITICAL RISK: Leaked `DATABASE_URL` (SEC-001)**
**Problem**: Storing `DATABASE_URL` in `.env` files is unacceptable for production. A single accidental commit could expose the entire database to the public internet.

**Mitigation & Implementation**:
1.  **Mandate a Production Secret Manager**: For self-hosting, Doppler is the recommended choice due to its ease of use and free tier. `.env` files are for local development **only**.
    *   **Action**: Add this to the story's acceptance criteria: "Production secrets, including `DATABASE_URL`, MUST be managed via Doppler (or an equivalent approved secret manager) and NOT from environment files."
2.  **Implement CI Secret Scanning**: Add a `gitleaks` workflow to the CI pipeline to scan for secrets on every push, preventing them from ever entering the Git history.
    *   **Action**: Create `.github/workflows/secret-scan.yml`:
        ```yaml
        name: Secret Scanning
        on: [push, pull_request]
        jobs:
          scan:
            name: Gitleaks Scan
            runs-on: ubuntu-latest
            steps:
              - uses: actions/checkout@v4
                with: { fetch-depth: 0 }
              - uses: gitleaks/gitleaks-action@v2
                env:
                  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
                  GITLEAKS_LICENSE: ${{ secrets.GITLEAKS_LICENSE }} # If you have a Pro license
        ```
3.  **Create a Secure Prisma Wrapper**: The Prisma client should be instantiated via a secure wrapper that validates the connection URL and prevents logging sensitive data in production.
    *   **Action**: Create `database/prisma/client.ts`:
        ```typescript
        import { PrismaClient } from '@prisma/client';

        function getPrismaClient(): PrismaClient {
          const databaseUrl = process.env.DATABASE_URL;
          if (!databaseUrl) {
            throw new Error('FATAL: DATABASE_URL environment variable is not set.');
          }

          if (process.env.NODE_ENV === 'production' && (databaseUrl.includes('localhost') || databaseUrl.includes('127.0.0.1'))) {
            throw new Error('FATAL: Cannot use localhost database in production.');
          }

          return new PrismaClient({
            datasources: { db: { url: databaseUrl } },
            log: process.env.NODE_ENV === 'development' ? ['query', 'info', 'warn', 'error'] : ['warn', 'error'],
          });
        }

        export const prisma = getPrismaClient();
        ```
    *   **Action**: All other parts of the application must import `prisma` from this file, not create their own client.

### **CRITICAL RISK: Catastrophic Migration Failure (OPS-001)**
**Problem**: A Prisma migration failing halfway through a production deployment can leave the database in a corrupted, inconsistent state, leading to extended downtime.

**Mitigation & Implementation**:
1.  **Mandate a Two-Phase Migration Process**: No migration should be run directly. It must be wrapped in a script that includes backup, dry-run, and rollback capabilities.
    *   **Action**: Create `scripts/run-migration.sh`:
        ```bash
        #!/bin/bash
        set -euo pipefail
        
        MIGRATION_NAME="$1"
        if [ -z "$MIGRATION_NAME" ]; then
          echo "‚ùå Error: Migration name is required."
          echo "Usage: ./scripts/run-migration.sh <migration_name>"
          exit 1
        fi

        echo "üì¶ Step 1: Backing up database..."
        TIMESTAMP=$(date +%Y%m%d_%H%M%S)
        BACKUP_FILE="backup_pre_${MIGRATION_NAME}_${TIMESTAMP}.sql"
        docker-compose exec -T postgres pg_dump -U ${POSTGRES_USER} -d ${POSTGRES_DB} > "$BACKUP_FILE"
        echo "‚úÖ Backup created at $BACKUP_FILE"

        echo "üß™ Step 2: Running migration in a transaction (dry-run)..."
        # This requires a more complex setup; for now, we rely on staging tests.
        # A true dry-run is a future improvement.

        echo "üöÄ Step 3: Applying migration..."
        pnpm --filter database prisma migrate deploy || {
          echo "‚ùå MIGRATION FAILED!"
          echo "üîô Rolling back from backup..."
          docker-compose exec -T postgres psql -U ${POSTGRES_USER} -d ${POSTGRES_DB} < "$BACKUP_FILE"
          echo "‚úÖ Rollback complete. Please investigate the failure."
          exit 1
        }
        echo "üéâ Migration successful."
        ```
    *   **Action**: The `DATABASE_MIGRATION_GUIDE.md` must be updated to mandate the use of this script for all deployments.

### **HIGH RISK: "ENUM Hell" (TECH-002)**
**Problem**: The schema uses native Prisma `enum`s for data that is likely to change (e.g., `Archetype`, `CompetitorCategory`). Adding a new archetype requires a developer to change code, create a new DB migration, and deploy the entire application. This is brittle and slow.

**Mitigation & Implementation**:
1.  **Adopt a Hybrid ENUM Strategy**: Use native Prisma `enum`s only for truly static, application-level states (`Platform`, `ApprovalStatus`). For business logic concepts that might evolve, use a related database table instead.
    *   **Action**: The architect MUST refactor the `schema.prisma`. For example, for `Archetype`:
        *   **Remove** the `Archetype` enum.
        *   **Create** a new `Archetype` model:
            ```prisma
            model Archetype {
              id          String @id @default(cuid())
              name        String @unique // e.g., "CHECKLIST", "MYTHBUST"
              description String
              isActive    Boolean @default(true)
              
              replies     Reply[]
              decisions   Decision[]

              @@map("archetypes")
            }
            ```
        *   **Update** the `Reply` and `Decision` models to use a foreign key relationship instead of the enum:
            ```prisma
            model Reply {
              // ...
              archetypeId String
              archetype   Archetype @relation(fields: [archetypeId], references: [id])
              // REMOVE: archetype   Archetype
            }
            ```
    *   **Action**: This refactoring pattern must be applied to `Archetype`, `PowerTier`, `CompetitorCategory`, `KpiType`, and `EscalationReason`. This allows new archetypes or KPIs to be added via the dashboard in the future without a code deployment.

### **HIGH RISK: Data Integrity Violation (DATA-001)**
**Problem**: A complex schema can have missing or incorrect constraints, leading to corrupt data (e.g., duplicate users, orphaned records).

**Mitigation & Implementation**:
1.  **Mandate Constraint Integration Tests**: Every unique constraint and important relation must have a dedicated test that proves it will reject invalid data.
    *   **Action**: Create `database/tests/constraints.test.ts` with a Vitest suite.
        ```typescript
        // database/tests/constraints.test.ts
        import { describe, it, expect } from 'vitest';
        import { prisma } from '../prisma/client';

        describe('Database Constraints', () => {
          it('should FAIL to create two authors with the same platform and platformId', async () => {
            const platformId = `test_user_${Date.now()}`;
            await prisma.author.create({
              data: { platform: 'TWITTER', platformId, handle: 'user1' },
            });
            
            // This should throw a unique constraint violation error
            const promise = prisma.author.create({
              data: { platform: 'TWITTER', platformId, handle: 'user2' },
            });

            await expect(promise).rejects.toThrow();
          });

          it('should FAIL to create a post with a non-existent authorId', async () => {
            const promise = prisma.post.create({
              data: {
                platform: 'TWITTER',
                platformPostId: 'post1',
                authorId: '00000000-0000-0000-0000-000000000000', // Fake UUID
                content: 'test content'
              }
            });
            await expect(promise).rejects.toThrow();
          });
        });
        ```
    *   **Action**: Add a task to Story 1.3: "Achieve 100% test coverage for all unique and foreign key constraints."

### **HIGH RISK: Query Performance (PERF-001)**
**Problem**: The schema defines indexes, but there is no process to verify they are being used correctly or are effective. A missing index on a core table could grind the application to a halt under load.

**Mitigation & Implementation**:
1.  **Mandate Query Analysis in PRs**: For any new feature that adds a database query, the Pull Request description **must** include the output of `EXPLAIN ANALYZE` for that query to prove it is efficient and using an index.
    *   **Action**: Add to `CONTRIBUTING.md`: "All PRs introducing new database queries must include the `EXPLAIN ANALYZE` output in the PR description."
    *   **Action**: Add a task to story 1.3: "Verify the performance of all initial indexes by running `EXPLAIN ANALYZE` on sample queries for `authors`, `posts`, and `decisions` and documenting the results."

---
## 4. Final Recommendation

Story 1.3 is **NO-GO**. The risks to data integrity, security, and long-term maintainability are too high. The Architect Agent must replace the existing `Dev Notes` with the detailed `Mitigation & Implementation` plan from this document. The schema refactoring to address "ENUM Hell" is a critical, non-negotiable change for the long-term health of the project.