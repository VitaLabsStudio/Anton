# Story 1.2: Docker Compose Infrastructure Setup

## Status: Draft

## Story

**As a** developer,  
**I want** Docker Compose configuration for all services with proper networking and persistence,  
**so that** the application can run reliably on self-hosted infrastructure with consistent environments.

## Acceptance Criteria

1. `docker-compose.yml` created in repo root with services: backend-api, backend-worker, dashboard, postgres, cloudflared
2. Dockerfile created for Node.js services with multi-stage build using **Node.js 24-alpine** base image:
   - Stage 1 (base): Node 24-alpine + pnpm 10.x via corepack
   - Stage 2 (deps): Install dependencies with frozen lockfile
   - Stage 3 (builder): Build TypeScript to JavaScript
   - Stage 4 (production): Copy dist + node_modules, expose port 3001, non-root user
   - Development target: Hot reload support
   - Production target: Optimized, minimal layers, security hardened
3. PostgreSQL service configured with specific version and persistent volume:
   - Image: `postgres:17-alpine` (latest stable, 30% faster JSON operations)
   - Persistent volume: `postgres_data` mounted to `/var/lib/postgresql/data`
   - Environment: POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_DB
   - Storage: 1TB available on host machine
   - Health check: `pg_isready -U antone` every 10s
   - Resource allocation: 8-16GB RAM for PostgreSQL (configurable based on workload)
   - Host specs: 32GB RAM total, Intel i5 6-core, 1TB SSD
4. Environment variables defined in `.env.example` (template for `.env`)
5. Health checks configured for all services with specific intervals:
   - PostgreSQL: `pg_isready -U antone` every 10s, timeout 5s, retries 5
   - Backend API: `curl -f http://localhost:3001/health` every 30s, timeout 10s, retries 3
   - Restart policy: `unless-stopped` for all services
6. Network configuration: Internal network for service communication, exposed ports for dashboard
7. Successful deployment locally with `docker-compose up -d`
8. All services healthy and communicating (postgres accessible from backend)
9. Data persists across container restarts (test with `docker-compose down && docker-compose up`)
10. **Cloudflare Tunnel service** configured:
    - Service: `cloudflared` with latest image
    - Command: `tunnel run`
    - Environment: `TUNNEL_TOKEN` from `.env`
    - Purpose: Secure remote access to dashboard without port forwarding
    - Configuration: `docker/cloudflared/config.yml` with ingress rules for dashboard and API
11. **Environment-specific compose files** created:
    - `docker-compose.yml`: Base configuration
    - `docker-compose.dev.yml`: Development overrides (hot reload, debug ports)
    - `docker-compose.prod.yml`: Production optimizations (resource limits, security)
    - Usage: `docker-compose -f docker-compose.yml -f docker-compose.dev.yml up`
12. **Backup Strategy** configured for disaster recovery:
    - Schedule: Automated daily backups at 2:00 AM local time
    - Destination: Backblaze B2 cloud storage (free 10GB tier)
    - Retention: 7-day rolling backups, monthly snapshots kept for 1 year
    - RTO (Recovery Time Objective): 4 hours from backup to operational
    - RPO (Recovery Point Objective): 24 hours maximum data loss
    - Backup verification: Weekly restore tests to local staging environment
    - Implementation: Script at `scripts/backup-db.sh` with cron scheduling

---

## Tasks / Subtasks

- [ ] **Task 1: Create Base Docker Compose File** (AC: 1, 6)
  - [ ] Create `docker/docker-compose.yml` with all services defined
  - [ ] Configure `antone-network` bridge network
  - [ ] Define `postgres_data` volume for persistence
  - [ ] Set service dependencies (backend depends on postgres, dashboard depends on backend)

- [ ] **Task 2: Create Backend Dockerfile** (AC: 2)
  - [ ] Create `backend/Dockerfile` with multi-stage build
  - [ ] Stage 1: Base with Node 24-alpine + pnpm via corepack
  - [ ] Stage 2: Dependencies with `pnpm install --frozen-lockfile`
  - [ ] Stage 3: Builder with TypeScript compilation
  - [ ] Stage 4: Production with minimal layers, non-root user
  - [ ] Add development target with hot reload support

- [ ] **Task 3: Create Dashboard Dockerfile** (AC: 2)
  - [ ] Create `dashboard/Dockerfile` with multi-stage build
  - [ ] Configure for Next.js 16 production build
  - [ ] Set `NEXT_PUBLIC_API_URL` environment variable
  - [ ] Expose port 3000

- [ ] **Task 4: Configure PostgreSQL Service** (AC: 3, 5)
  - [ ] Use `postgres:17-alpine` image
  - [ ] Mount `postgres_data` volume
  - [ ] Configure environment variables from .env
  - [ ] Add health check: `pg_isready -U antone` every 10s

- [ ] **Task 5: Configure Backend API Service** (AC: 5)
  - [ ] Build from `backend/Dockerfile` with production target
  - [ ] Set DATABASE_URL environment variable
  - [ ] Expose port 3001
  - [ ] Add health check: `curl -f http://localhost:3001/health`
  - [ ] Depend on postgres with condition `service_healthy`

- [ ] **Task 6: Configure Backend Worker Service** (AC: 1)
  - [ ] Build from `backend/Dockerfile` with production target
  - [ ] Override command to `node dist/workers/index.js`
  - [ ] No exposed ports (internal only)
  - [ ] Depend on postgres and backend-api

- [ ] **Task 7: Configure Dashboard Service** (AC: 1)
  - [ ] Build from `dashboard/Dockerfile`
  - [ ] Expose port 3000
  - [ ] Set NEXT_PUBLIC_API_URL to backend-api

- [ ] **Task 8: Configure Cloudflared Service** (AC: 10)
  - [ ] Use `cloudflare/cloudflared:latest` image
  - [ ] Command: `tunnel run`
  - [ ] Environment: TUNNEL_TOKEN from .env
  - [ ] Create `docker/cloudflared/config.yml` with ingress rules

- [ ] **Task 9: Create Environment Template** (AC: 4)
  - [ ] Create `.env.example` with all required variables
  - [ ] Include PostgreSQL credentials
  - [ ] Include platform API keys (placeholders)
  - [ ] Include Cloudflare tunnel token
  - [ ] Add comprehensive comments explaining each variable

- [ ] **Task 10: Create Development Override File** (AC: 11)
  - [ ] Create `docker/docker-compose.dev.yml`
  - [ ] Enable hot reload for backend (mount source code)
  - [ ] Expose debug ports (9229 for Node inspector)
  - [ ] Use development Dockerfile target

- [ ] **Task 11: Create Production Override File** (AC: 11)
  - [ ] Create `docker/docker-compose.prod.yml`
  - [ ] Add resource limits (memory, CPU)
  - [ ] Enable security hardening
  - [ ] Configure log rotation

- [ ] **Task 12: Create Backup Script** (AC: 12)
  - [ ] Create `scripts/backup-db.sh`
  - [ ] Use `pg_dump` for database backup
  - [ ] Compress backup with gzip
  - [ ] Upload to Backblaze B2 (or local backup if B2 not configured)
  - [ ] Implement 7-day rolling retention

- [ ] **Task 13: Create Restore Script** (AC: 12)
  - [ ] Create `scripts/restore-db.sh`
  - [ ] Download backup from B2 or local
  - [ ] Decompress and restore with `pg_restore`
  - [ ] Verify restoration with health check

- [ ] **Task 14: Test Deployment** (AC: 7, 8, 9)
  - [ ] Run `docker-compose up -d`
  - [ ] Verify all services are healthy
  - [ ] Test postgres connectivity from backend
  - [ ] Test data persistence with restart cycle

---

## Dev Notes

### Previous Story Insights
Story 1.1 must be completed first - this story depends on the monorepo structure and package.json files created in 1.1.

### Docker Compose Configuration
[Source: architecture.md#11-infrastructure-deployment]

```yaml
# docker/docker-compose.yml
version: '3.8'

services:
  postgres:
    image: postgres:17-alpine
    container_name: antone-postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-antone}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB:-antone}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U antone"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    networks:
      - antone-network

  backend-api:
    build:
      context: ../backend
      dockerfile: Dockerfile
      target: production
    container_name: antone-backend-api
    environment:
      NODE_ENV: production
      DATABASE_URL: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
    env_file:
      - ../.env
    ports:
      - "3001:3001"
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - antone-network

  backend-worker:
    build:
      context: ../backend
      dockerfile: Dockerfile
      target: production
    container_name: antone-backend-worker
    command: ["node", "dist/workers/index.js"]
    environment:
      NODE_ENV: production
      DATABASE_URL: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
    env_file:
      - ../.env
    depends_on:
      postgres:
        condition: service_healthy
      backend-api:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - antone-network

  dashboard:
    build:
      context: ../dashboard
      dockerfile: Dockerfile
    container_name: antone-dashboard
    environment:
      NEXT_PUBLIC_API_URL: http://backend-api:3001
    ports:
      - "3000:3000"
    depends_on:
      - backend-api
    restart: unless-stopped
    networks:
      - antone-network

  cloudflared:
    image: cloudflare/cloudflared:latest
    container_name: antone-cloudflared
    command: tunnel run
    environment:
      TUNNEL_TOKEN: ${CLOUDFLARE_TUNNEL_TOKEN}
    restart: unless-stopped
    networks:
      - antone-network

volumes:
  postgres_data:
    driver: local

networks:
  antone-network:
    driver: bridge
```

### Backend Dockerfile
[Source: architecture.md#11.2-dockerfile-backend]

```dockerfile
# backend/Dockerfile

FROM node:24-alpine AS base
WORKDIR /app
RUN corepack enable && corepack prepare pnpm@10.0.0 --activate

# Dependencies stage
FROM base AS deps
COPY package.json pnpm-lock.yaml ./
RUN pnpm install --frozen-lockfile

# Build stage
FROM base AS builder
COPY --from=deps /app/node_modules ./node_modules
COPY . .
RUN pnpm build

# Production stage
FROM base AS production
ENV NODE_ENV=production
RUN addgroup --system --gid 1001 nodejs && \
    adduser --system --uid 1001 antone
COPY --from=builder /app/dist ./dist
COPY --from=builder /app/node_modules ./node_modules
COPY package.json ./
USER antone
EXPOSE 3001
CMD ["node", "dist/index.js"]
```

### Cloudflare Tunnel Configuration
[Source: architecture.md#11.3-cloudflare-tunnel-configuration]

```yaml
# docker/cloudflared/config.yml
tunnel: antone-tunnel
credentials-file: /etc/cloudflared/credentials.json

ingress:
  - hostname: antone.yourdomain.com
    service: http://dashboard:3000
  - hostname: api.antone.yourdomain.com
    service: http://backend-api:3001
  - service: http_status:404
```

### Environment Variables Template
[Source: architecture.md#10.2-secrets-management]

```bash
# .env.example

# PostgreSQL Configuration
POSTGRES_USER=antone
POSTGRES_PASSWORD=your_secure_password_here
POSTGRES_DB=antone

# Platform API Keys (Twitter)
TWITTER_API_KEY=
TWITTER_API_SECRET=
TWITTER_ACCESS_TOKEN=
TWITTER_ACCESS_SECRET=
TWITTER_BEARER_TOKEN=

# Platform API Keys (Reddit)
REDDIT_CLIENT_ID=
REDDIT_CLIENT_SECRET=
REDDIT_REFRESH_TOKEN=

# Platform API Keys (Threads)
THREADS_ACCESS_TOKEN=

# LLM API
DEEPSEEK_API_KEY=

# Database URL (auto-constructed in Docker)
DATABASE_URL=postgresql://antone:your_secure_password_here@postgres:5432/antone

# Authentication
JWT_SECRET=your_jwt_secret_here

# External Services
HEALTHCHECKS_IO_UUID=
CLOUDFLARE_TUNNEL_TOKEN=

# Backup Configuration
BACKBLAZE_KEY_ID=
BACKBLAZE_APPLICATION_KEY=
BACKBLAZE_BUCKET_NAME=antone-backups
```

### File Structure
[Source: architecture.md#3-repository-structure]

```
docker/
├── docker-compose.yml          # Base configuration
├── docker-compose.dev.yml      # Development overrides
├── docker-compose.prod.yml     # Production overrides
└── cloudflared/
    └── config.yml              # Cloudflare Tunnel config

scripts/
├── backup-db.sh               # Database backup script
└── restore-db.sh              # Database restore script
```

---

## Testing

### Test File Location
Integration tests for Docker setup in `backend/tests/integration/`

### Testing Standards
- Verify all containers start successfully
- Verify health checks pass
- Test network connectivity between services
- Test data persistence across restarts

### Story-Specific Testing Requirements
1. Run `docker-compose up -d` and verify all services healthy
2. Connect to postgres from backend container
3. Stop and restart containers, verify data persists
4. Test Cloudflare tunnel connectivity (if configured)

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2024-12-01 | 1.0 | Initial story draft | Bob (SM Agent) |

---

## Dev Agent Record

### Agent Model Used
*To be filled by Dev Agent*

### Debug Log References
*To be filled by Dev Agent*

### Completion Notes List
*To be filled by Dev Agent*

### File List
*To be filled by Dev Agent*

---

## QA Results
*To be filled by QA Agent*

