# Story 1.7: Stream Monitor Worker with Maximum Reach Filtering & Reddit Karma Strategy

## Status: Draft

## Story

**As** the system,  
**I want** a persistent background worker with maximum reach keyword filtering (200+ terms), permissive processing, and strategic Reddit karma farming,  
**so that** we reach the maximum number of potential customers (1,500-2,500/week) while building Reddit credibility for long-term engagement.

## Acceptance Criteria

1. Worker process created at `@backend/workers/stream-monitor.ts`
2. **Tier 1: Expanded Keyword Taxonomy** (200+ keywords across 9 categories):
   - Category 1 - Direct Hangover (10 terms)
   - Category 2 - Physical Symptoms PRIMARY (30 terms)
   - Category 3 - Physical Symptoms SECONDARY (25 terms)
   - Category 4 - Cognitive/Emotional (35 terms)
   - Category 5 - Drinking Context (40 terms)
   - Category 6 - Recovery Intent (40 terms)
   - Category 7 - Wellness Context (30 terms)
   - Category 8 - Slang (25 terms)
   - Category 9 - Meme Language (15 terms)
3. **Platform-specific keyword searches** (maximum breadth)
4. **Tier 2: Minimal Spam Filtering** (permissive by default)
5. **No scoring threshold** - all keyword matches that pass spam filter go to database
6. **Polling logic with temporal intelligence**
7. All filtered posts written to `posts` table with metadata
8. **Duplicate detection** implemented efficiently
9. Worker logs comprehensive metrics
10. **Worker architecture implemented** with centralized WorkerManager
11. **Graceful degradation** implemented for platform failures
12. Worker runs successfully for 1 hour with production-like volume
13. Logs show detailed funnel metrics
14. Monitoring dashboard shows real-time metrics
15. **Configuration file structure** at `@backend/config/keywords.json`
16. **Reddit Karma Farming Strategy** implemented

---

## Tasks / Subtasks

- [ ] **Task 1: Create Keywords Configuration** (AC: 2, 15)
  - [ ] Create `backend/src/config/keywords.json`
  - [ ] Add Category 1: Direct Hangover (10 terms)
  - [ ] Add Category 2: Physical Symptoms PRIMARY (30 terms)
  - [ ] Add Category 3: Physical Symptoms SECONDARY (25 terms)
  - [ ] Add Category 4: Cognitive/Emotional (35 terms)
  - [ ] Add Category 5: Drinking Context (40 terms)
  - [ ] Add Category 6: Recovery Intent (40 terms)
  - [ ] Add Category 7: Wellness Context (30 terms)
  - [ ] Add Category 8: Slang (25 terms)
  - [ ] Add Category 9: Meme Language (15 terms)
  - [ ] Add exclusions array for spam patterns
  - [ ] Add version field for tracking changes

- [ ] **Task 2: Create Spam Filter** (AC: 4)
  - [ ] Create `backend/src/workers/spam-filter.ts`
  - [ ] Filter movie/music reviews ("The Hangover" capitalized)
  - [ ] Filter cryptocurrency mentions
  - [ ] Filter brand accounts (verified + >50k followers)
  - [ ] Filter link spam (>5 URLs)
  - [ ] Default: pass through borderline cases

- [ ] **Task 3: Create Temporal Intelligence** (AC: 6)
  - [ ] Create `backend/src/workers/temporal-multiplier.ts`
  - [ ] Implement `getTemporalMultiplier()` function
  - [ ] Sunday 6-11am: 3× frequency
  - [ ] Saturday 6-11am: 2× frequency
  - [ ] Friday-Saturday night: 1.5× frequency
  - [ ] Normal times: 1× frequency

- [ ] **Task 4: Create Twitter Monitor** (AC: 3, 7)
  - [ ] Create `backend/src/platforms/twitter/monitor.ts`
  - [ ] Build compound query from 200+ keywords
  - [ ] Use `sinceId` for duplicate prevention
  - [ ] Fetch tweet fields: created_at, public_metrics, author_id
  - [ ] Save matching posts to database
  - [ ] Log keyword matches and categories

- [ ] **Task 5: Create Reddit Monitor** (AC: 3, 7, 16)
  - [ ] Create `backend/src/platforms/reddit/monitor.ts`
  - [ ] Monitor core subreddits: r/hangover, r/stopdrinking, r/drunk, r/alcohol
  - [ ] Monitor health subreddits: r/AskDocs, r/HealthAnxiety, r/medical_advice
  - [ ] Monitor lifestyle: r/AskReddit, r/CasualConversation, r/college
  - [ ] Track last processed timestamp per subreddit
  - [ ] Save matching posts to database
  - [ ] Implement karma gate logic

- [ ] **Task 6: Create Threads Monitor** (AC: 3, 7)
  - [ ] Create `backend/src/platforms/threads/monitor.ts`
  - [ ] Search hashtags: #hangover, #morningafter, #sundayscaries
  - [ ] Search keywords from taxonomy
  - [ ] Track last processed thread ID
  - [ ] Handle API unavailable gracefully

- [ ] **Task 7: Create Duplicate Detection** (AC: 8)
  - [ ] Track last processed post ID per platform
  - [ ] Use `sinceId` for Twitter
  - [ ] Use timestamp for Reddit
  - [ ] Use thread ID for Threads
  - [ ] Resume from last processed on restart

- [ ] **Task 8: Create Stream Monitor Worker** (AC: 1, 6, 9, 12, 13)
  - [ ] Create `backend/src/workers/stream-monitor.ts`
  - [ ] Initialize platform monitors
  - [ ] Implement polling loop with temporal intelligence
  - [ ] Aggregate results from all platforms
  - [ ] Apply spam filter to results
  - [ ] Save filtered posts to database
  - [ ] Log comprehensive funnel metrics

- [ ] **Task 9: Create Worker Manager** (AC: 10)
  - [ ] Create `backend/src/workers/index.ts`
  - [ ] Implement WorkerManager class
  - [ ] Support 5 concurrent workers (StreamMonitor + placeholders)
  - [ ] Implement graceful shutdown (SIGTERM handling)
  - [ ] Error isolation per worker
  - [ ] Entry point: `WorkerManager.start()`

- [ ] **Task 10: Implement Graceful Degradation** (AC: 11)
  - [ ] Track platform status: healthy | degraded | down
  - [ ] Continue with working platforms if one fails
  - [ ] Alert CRITICAL if all platforms fail
  - [ ] Increase poll interval for failed platforms
  - [ ] Retry via circuit breaker half-open state

- [ ] **Task 11: Implement Reddit Karma Strategy** (AC: 16)
  - [ ] Track u/antone_vita karma score
  - [ ] Implement karma gate (500+ before product mentions)
  - [ ] Phase 1: Pure value mode for first 2 weeks
  - [ ] Maintain 30% value-only ratio after gate
  - [ ] Prioritize high-traffic subreddits

- [ ] **Task 12: Create Metrics Logging** (AC: 9, 13, 14)
  - [ ] Log posts scanned per platform
  - [ ] Log keyword matches per category
  - [ ] Log spam filtered count
  - [ ] Log posts queued for analysis
  - [ ] Calculate and log funnel metrics

- [ ] **Task 13: Write Integration Tests** (AC: 12)
  - [ ] Test worker runs for extended period
  - [ ] Test with mocked platform responses
  - [ ] Verify posts saved to database
  - [ ] Test graceful degradation
  - [ ] Test temporal multiplier

---

## Dev Notes

### Previous Story Insights
- Stories 1.4-1.6 must be complete (platform clients)
- Reuse TwitterClient, RedditClient, ThreadsClient
- Reuse RateLimiter and CircuitBreaker utilities

### Keywords Configuration
[Source: epic-1-foundation-core-infrastructure.md#story-1.7]

```json
// backend/src/config/keywords.json
{
  "version": "1.0",
  "categories": {
    "direct_hangover": {
      "weight": 1.0,
      "terms": [
        "hangover", "hungover", "morning after", "day after drinking",
        "after party", "hair of the dog", "walk of shame",
        "death warmed over", "rough morning", "rough night"
      ]
    },
    "physical_symptoms_primary": {
      "weight": 1.0,
      "terms": [
        "nausea", "nauseous", "queasy", "feel sick", "stomach hurts",
        "vomit", "vomiting", "throw up", "puking", "dry heaving",
        "can't keep anything down", "bile", "headache", "migraine",
        "pounding head", "splitting headache", "head hurts",
        "head is killing me", "temples throbbing", "sensitivity to light",
        "photophobia", "dehydrated", "dehydration", "dry mouth",
        "cotton mouth", "parched", "dizzy", "dizziness", "vertigo",
        "room spinning", "lightheaded", "shaking", "tremors", "sweats"
      ]
    },
    // ... additional categories
  },
  "exclusions": [
    "The Hangover",
    "soundtrack",
    "album",
    "song",
    "bitcoin",
    "ethereum",
    "crypto"
  ]
}
```

### Stream Monitor Implementation
[Source: architecture.md#4.3-backend-worker-architecture]

```typescript
// backend/src/workers/stream-monitor.ts

import { TwitterClient } from '../platforms/twitter/client';
import { RedditClient } from '../platforms/reddit/client';
import { ThreadsClient } from '../platforms/threads/client';
import { prisma } from '../db';
import { logger } from '../utils/logger';
import { loadKeywords } from '../config/keywords';
import { SpamFilter } from './spam-filter';
import { getTemporalMultiplier } from './temporal-multiplier';

export class StreamMonitor {
  private twitter: TwitterClient;
  private reddit: RedditClient;
  private threads: ThreadsClient;
  private keywords: KeywordConfig;
  private spamFilter: SpamFilter;
  private lastProcessedIds: Map<string, string>;
  private platformStatus: Map<string, 'healthy' | 'degraded' | 'down'>;

  constructor() {
    this.twitter = new TwitterClient();
    this.reddit = new RedditClient();
    this.threads = new ThreadsClient();
    this.keywords = loadKeywords();
    this.spamFilter = new SpamFilter(this.keywords.exclusions);
    this.lastProcessedIds = new Map();
    this.platformStatus = new Map([
      ['twitter', 'healthy'],
      ['reddit', 'healthy'],
      ['threads', 'healthy'],
    ]);
  }

  async start(): Promise<void> {
    logger.info('Stream Monitor starting');
    
    while (true) {
      try {
        await this.scanCycle();
      } catch (error) {
        logger.error('Scan cycle failed', { error });
      }

      const baseInterval = 15 * 60 * 1000; // 15 minutes
      const multiplier = getTemporalMultiplier();
      const interval = baseInterval / multiplier;
      
      logger.info(`Next scan in ${interval / 1000}s (multiplier: ${multiplier})`);
      await this.sleep(interval);
    }
  }

  private async scanCycle(): Promise<void> {
    const metrics = {
      twitter: { scanned: 0, matched: 0, spam: 0, queued: 0 },
      reddit: { scanned: 0, matched: 0, spam: 0, queued: 0 },
      threads: { scanned: 0, matched: 0, spam: 0, queued: 0 },
    };

    // Scan platforms in parallel
    const [twitterPosts, redditPosts, threadsPosts] = await Promise.allSettled([
      this.scanTwitter(),
      this.scanReddit(),
      this.scanThreads(),
    ]);

    // Process results
    // ... aggregate and log metrics

    this.logFunnelMetrics(metrics);
  }

  private async scanTwitter(): Promise<DetectedPost[]> {
    if (this.platformStatus.get('twitter') === 'down') {
      return [];
    }

    try {
      const query = this.buildTwitterQuery();
      const tweets = await this.twitter.search(query, {
        maxResults: 100,
        sinceId: this.lastProcessedIds.get('twitter'),
      });

      const posts: DetectedPost[] = [];
      for (const tweet of tweets) {
        const matches = this.matchKeywords(tweet.text);
        if (matches.length > 0 && !this.spamFilter.isSpam(tweet)) {
          posts.push({
            platform: 'TWITTER',
            platformPostId: tweet.id,
            content: tweet.text,
            authorId: tweet.author_id,
            keywordMatches: matches.keywords,
            keywordCategories: matches.categories,
            rawMetrics: tweet.public_metrics,
          });
        }
      }

      if (tweets.length > 0) {
        this.lastProcessedIds.set('twitter', tweets[0].id);
      }

      this.platformStatus.set('twitter', 'healthy');
      return posts;
    } catch (error) {
      this.handlePlatformFailure('twitter', error);
      return [];
    }
  }

  private logFunnelMetrics(metrics: ScanMetrics): void {
    const total = {
      scanned: metrics.twitter.scanned + metrics.reddit.scanned + metrics.threads.scanned,
      matched: metrics.twitter.matched + metrics.reddit.matched + metrics.threads.matched,
      spam: metrics.twitter.spam + metrics.reddit.spam + metrics.threads.spam,
      queued: metrics.twitter.queued + metrics.reddit.queued + metrics.threads.queued,
    };

    logger.info('Scan cycle complete', {
      twitter: metrics.twitter,
      reddit: metrics.reddit,
      threads: metrics.threads,
      total,
      reduction: `${((1 - total.queued / total.matched) * 100).toFixed(1)}%`,
    });
  }
}
```

### Temporal Multiplier
```typescript
// backend/src/workers/temporal-multiplier.ts

export function getTemporalMultiplier(): number {
  const now = new Date();
  const day = now.getDay(); // 0 = Sunday, 6 = Saturday
  const hour = now.getHours();

  // Sunday 6-11am: Peak suffering
  if (day === 0 && hour >= 6 && hour < 11) {
    return 3;
  }

  // Saturday 6-11am: Secondary peak
  if (day === 6 && hour >= 6 && hour < 11) {
    return 2;
  }

  // Friday-Saturday night (6pm-2am)
  if ((day === 5 || day === 6) && (hour >= 18 || hour < 2)) {
    return 1.5;
  }

  // Normal times
  return 1;
}
```

### Spam Filter
```typescript
// backend/src/workers/spam-filter.ts

export class SpamFilter {
  private exclusions: string[];

  constructor(exclusions: string[]) {
    this.exclusions = exclusions;
  }

  isSpam(post: any): boolean {
    const content = post.content || post.text || '';
    
    // Movie/music reviews
    if (/The Hangover/i.test(content) && /(movie|film|soundtrack|album|song)/i.test(content)) {
      return true;
    }

    // Cryptocurrency
    if (/(bitcoin|ethereum|crypto)/i.test(content) && /(\$|USD|crash|moon)/i.test(content)) {
      return true;
    }

    // Brand accounts (verified + high followers)
    if (post.author?.verified && post.author?.followers > 50000) {
      return true;
    }

    // Link spam (>5 URLs)
    const urlCount = (content.match(/https?:\/\//g) || []).length;
    if (urlCount > 5) {
      return true;
    }

    return false;
  }
}
```

### Worker Manager
```typescript
// backend/src/workers/index.ts

import { StreamMonitor } from './stream-monitor';
import { logger } from '../utils/logger';

export class WorkerManager {
  private workers: Map<string, { worker: any; running: boolean }>;
  private shutdownRequested: boolean = false;

  constructor() {
    this.workers = new Map();
    this.setupShutdownHandlers();
  }

  async start(): Promise<void> {
    logger.info('WorkerManager starting');

    // Initialize workers
    this.workers.set('stream-monitor', {
      worker: new StreamMonitor(),
      running: false,
    });

    // Placeholder for future workers
    // this.workers.set('queue-processor', ...);
    // this.workers.set('feedback-collector', ...);

    // Start all workers concurrently
    const startPromises = Array.from(this.workers.entries()).map(
      async ([name, { worker }]) => {
        try {
          this.workers.get(name)!.running = true;
          await worker.start();
        } catch (error) {
          logger.error(`Worker ${name} failed`, { error });
          this.workers.get(name)!.running = false;
        }
      }
    );

    await Promise.allSettled(startPromises);
  }

  private setupShutdownHandlers(): void {
    process.on('SIGTERM', async () => {
      logger.info('SIGTERM received, initiating graceful shutdown');
      this.shutdownRequested = true;
      await this.stop();
      process.exit(0);
    });

    process.on('SIGINT', async () => {
      logger.info('SIGINT received, initiating graceful shutdown');
      this.shutdownRequested = true;
      await this.stop();
      process.exit(0);
    });
  }

  async stop(): Promise<void> {
    logger.info('Stopping all workers');
    // Allow current work to complete
    await new Promise(resolve => setTimeout(resolve, 5000));
  }
}

// Entry point
if (require.main === module) {
  const manager = new WorkerManager();
  manager.start().catch(error => {
    logger.error('WorkerManager failed to start', { error });
    process.exit(1);
  });
}
```

### Reddit Karma Strategy
[Source: epic-1-foundation-core-infrastructure.md#story-1.7]

```typescript
// Phase 1 (First 2 weeks): Pure value mode
// - NO product mentions or links in any Reddit replies
// - Focus on helpful, educational content only
// - Target: Build 500+ karma before product mentions begin

// Karma Gate Logic
async function shouldIncludeProduct(): Promise<boolean> {
  const karma = await redditClient.getKarma();
  
  if (karma < 500) {
    return false; // Pure value mode
  }
  
  // After 500 karma, 30% of replies are still value-only
  return Math.random() > 0.3;
}
```

### File Structure
```
backend/src/
├── config/
│   └── keywords.json            # 200+ keyword taxonomy
├── workers/
│   ├── index.ts                 # WorkerManager entry point
│   ├── stream-monitor.ts        # Main monitoring worker
│   ├── spam-filter.ts           # Spam detection
│   └── temporal-multiplier.ts   # Time-based frequency
├── platforms/
│   ├── twitter/
│   │   └── monitor.ts           # Twitter-specific monitoring
│   ├── reddit/
│   │   └── monitor.ts           # Reddit-specific monitoring
│   └── threads/
│       └── monitor.ts           # Threads-specific monitoring
```

---

## Testing

### Test File Location
- `backend/tests/unit/workers/stream-monitor.test.ts`
- `backend/tests/unit/workers/spam-filter.test.ts`
- `backend/tests/unit/workers/temporal-multiplier.test.ts`
- `backend/tests/integration/workers/stream-monitor.test.ts`

### Testing Standards
- Mock all platform API calls
- Test keyword matching accuracy
- Test spam filter rules
- Test temporal multiplier at different times
- Test graceful degradation

### Story-Specific Testing Requirements
1. Worker starts and runs polling loop
2. Keywords match expected posts
3. Spam filter excludes correct content
4. Posts are saved to database
5. Metrics are logged correctly
6. Graceful shutdown works

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2024-12-01 | 1.0 | Initial story draft | Bob (SM Agent) |

---

## Dev Agent Record

### Agent Model Used
*To be filled by Dev Agent*

### Debug Log References
*To be filled by Dev Agent*

### Completion Notes List
*To be filled by Dev Agent*

### File List
*To be filled by Dev Agent*

---

## QA Results
*To be filled by QA Agent*

